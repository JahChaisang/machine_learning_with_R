---
title: "Logistic Regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this example, we will perform decision tree model fitting on iris dataset example.

```{r}
library(ggplot2)
library(lattice)
library(caret)
data(iris)
head(iris)
```

```{r}
preprocessParams1 <- preProcess(iris[,1:4], method=c('center', 'scale'))
iris[,1:4] <- predict(preprocessParams1, iris[,1:4])
head(iris)
```

## Create decision tree model
```{r}
library(rpart)
dTree <- rpart(Species ~., data = iris, method="class")
pred = predict(dTree, iris, type = "class")
result = data.frame(label=iris$Species, prediction=pred)
head(result)
```

```{r}
confMat <- table(iris$Species, pred)
accuracy <- sum(diag(confMat))/sum(confMat)
print(accuracy)
```

## Training decision tree using Caret

```{r}
control <- trainControl(method="cv", number=3)
model <- train(Species~., data=iris, method="rpart", trControl=control)
model
```

Note that cp controls complexity of the tree. The higher cp, the more likely decision tree will be overfitted due to the trees having too many branches and leaves.

## Training knn

Just for the fun of it, let's also fit knn classification and compare to other methods.

```{r}
model <- train(Species~., data=iris, method="knn", trControl=control)
model
```

## KMeans Clustering

Let's also play a little bit with kmeans clustering in R.

```{r}
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
```


```{r}
irisCluster <- kmeans(iris[, 3:4], 3, nstart = 20)
irisCluster
```
